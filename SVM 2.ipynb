{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da63b13e-5bc3-49e8-8d33-c130f74a34ba",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebbb27e-7ac6-4cf7-801d-87df602ce1a2",
   "metadata": {},
   "source": [
    "Polynomial functions and kernel functions are related in the context of machine learning, particularly in support vector machines (SVMs) and kernelized methods. Kernel functions play a crucial role in allowing linear algorithms to operate in a higher-dimensional space without explicitly computing the transformation.\n",
    "\n",
    "In SVMs, the basic idea is to find a hyperplane that separates different classes in the input space. However, in some cases, the data might not be linearly separable in the original space. This is where kernel functions come into play.\n",
    "\n",
    "A polynomial kernel is a type of kernel function that computes the dot product of the transformed input vectors in a higher-dimensional space. The general form of a polynomial kernel is given by:\n",
    "\n",
    "\\[ K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i^T \\mathbf{x}_j + c)^d \\]\n",
    "\n",
    "Here,\n",
    "- \\( \\mathbf{x}_i \\) and \\( \\mathbf{x}_j \\) are input vectors.\n",
    "- \\( c \\) is a constant term.\n",
    "- \\( d \\) is the degree of the polynomial.\n",
    "\n",
    "This kernel allows SVMs to model non-linear relationships in the input space by implicitly mapping the input features into a higher-dimensional space.\n",
    "\n",
    "In summary, polynomial functions are used as kernel functions in machine learning algorithms, specifically in SVMs, to handle non-linear relationships in the data. The polynomial kernel enables the SVM to operate in a higher-dimensional feature space without explicitly computing the transformation, making it computationally efficient. Other kernel functions, such as radial basis function (RBF) kernels, are also commonly used for similar purposes in machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a599fc9a-ce8f-4a6c-ad7f-8970da368259",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61badafb-5a26-489f-bc4b-c7c14c83e264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load a sample dataset (e.g., the Iris dataset)\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM with a polynomial kernel\n",
    "# Specify the 'poly' kernel and set the degree parameter\n",
    "# You can also adjust other parameters such as C (regularization parameter)\n",
    "svm_classifier = SVC(kernel='poly', degree=3, C=1.0)\n",
    "\n",
    "# Train the SVM on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627cab10-4d22-467f-9450-1704364046d7",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f83201-630a-4f0e-9324-9eb1f906f84c",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), the parameter \\( \\epsilon \\) (epsilon) is a critical tuning parameter that determines the width of the margin within which no penalty is associated with errors. In SVR, the goal is to find a hyperplane that captures the majority of the data points within this margin. Points outside the margin are penalized based on their distance from the hyperplane.\n",
    "\n",
    "Here's how the value of \\( \\epsilon \\) affects the number of support vectors in SVR:\n",
    "\n",
    "1. **Smaller \\( \\epsilon \\):** When \\( \\epsilon \\) is small, the margin is narrow, and the SVR model becomes more sensitive to individual data points. As a result, more data points may fall outside the margin, leading to a higher number of support vectors. A smaller \\( \\epsilon \\) allows the model to fit the training data more closely, which can result in a more flexible but potentially overfit model.\n",
    "\n",
    "2. **Larger \\( \\epsilon \\):** When \\( \\epsilon \\) is large, the margin becomes wider, and the SVR model becomes less sensitive to individual data points. In this case, fewer data points are treated as support vectors, as the margin is more forgiving, and the model focuses on capturing the overall trend rather than fitting each data point precisely. A larger \\( \\epsilon \\) tends to produce a more generalized model with better generalization to unseen data.\n",
    "\n",
    "In summary, the choice of \\( \\epsilon \\) in SVR impacts the trade-off between model flexibility and generalization. A smaller \\( \\epsilon \\) allows the model to fit the training data more closely, potentially leading to overfitting, while a larger \\( \\epsilon \\) encourages a more generalized model. The actual impact on the number of support vectors depends on the specific characteristics of the data and the relationship between \\( \\epsilon \\) and the margin width. It's often a good practice to tune \\( \\epsilon \\) along with other hyperparameters using techniques such as cross-validation to find the optimal values for the given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba901be7-2ecf-4df2-8792-dc139ebaafd2",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391e6653-cfdd-45fb-99ab-e4d1e1f20e12",
   "metadata": {},
   "source": [
    "Support Vector Regression (SVR) is a machine learning algorithm used for regression tasks. The performance of SVR is highly influenced by its hyperparameters. Let's discuss how the choice of kernel function, \\( C \\) parameter, \\( \\epsilon \\) parameter, and \\( \\gamma \\) parameter can affect SVR's performance:\n",
    "\n",
    "1. **Kernel Function:**\n",
    "   - **Linear Kernel (`kernel='linear'`):** Suitable for linear relationships. If the relationship between the features and the target variable is approximately linear, a linear kernel can be effective.\n",
    "   - **RBF (Radial Basis Function) Kernel (`kernel='rbf'`):** Suitable for non-linear relationships. It introduces a parameter \\( \\gamma \\) that controls the shape of the decision boundary.\n",
    "   - **Polynomial Kernel (`kernel='poly'`):** Suitable for polynomial relationships. It introduces a parameter \\( \\degree \\) to control the degree of the polynomial.\n",
    "\n",
    "   **Example:**\n",
    "   - If the data has a complex, non-linear relationship, you might choose the RBF or polynomial kernel. Experiment with both and see which one performs better through cross-validation.\n",
    "\n",
    "2. **C Parameter:**\n",
    "   - **C parameter (`C`):** Controls the trade-off between achieving a low training error and a low testing error. It acts as a regularization parameter.\n",
    "  \n",
    "   **Example:**\n",
    "   - A smaller \\( C \\) (e.g., 0.1) allows for a larger margin and might generalize better to unseen data. A larger \\( C \\) (e.g., 10) results in a smaller margin but fits the training data more closely.\n",
    "\n",
    "3. **Epsilon Parameter:**\n",
    "   - **Epsilon parameter (`epsilon`):** Determines the width of the margin where no penalty is applied to errors. It defines a tube around the regression line within which errors are not penalized.\n",
    "\n",
    "   **Example:**\n",
    "   - A smaller \\( \\epsilon \\) (e.g., 0.1) results in a narrow tube, making the model sensitive to errors. A larger \\( \\epsilon \\) (e.g., 1.0) widens the tube, allowing for more flexibility and potentially reducing overfitting.\n",
    "\n",
    "4. **Gamma Parameter:**\n",
    "   - **Gamma parameter (`gamma`):** Defines how far the influence of a single training example reaches. It affects the shape of the decision boundary in the case of the RBF kernel.\n",
    "\n",
    "   **Example:**\n",
    "   - Smaller \\( \\gamma \\) (e.g., 0.01) results in a broader decision boundary, making the model less sensitive to individual data points. Larger \\( \\gamma \\) (e.g., 1.0) makes the model more focused on individual data points, potentially leading to overfitting.\n",
    "\n",
    "In practice, it's common to perform a hyperparameter search using techniques like grid search or randomized search, coupled with cross-validation, to find the combination of parameters that optimizes the model's performance on a specific dataset. The optimal values may vary depending on the characteristics of the data and the nature of the relationship between features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653fd558-6d50-4087-8fa8-39dbbe051aad",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3267bccf-f7a0-4939-bd4a-3cbcb1bfdaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Best Hyperparameters: {'C': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tuned_svm_classifier.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib  # for saving the trained model\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels on the testing data\n",
    "y_pred = svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance using accuracy as an example metric\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Tune hyperparameters using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10, 100]}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best Hyperparameters: {best_params}')\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "tuned_svm_classifier = SVC(**best_params)\n",
    "tuned_svm_classifier.fit(X, y)\n",
    "\n",
    "# Save the trained classifier to a file for future use\n",
    "joblib.dump(tuned_svm_classifier, 'tuned_svm_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6852d80e-2aab-450c-a819-da7340114e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
